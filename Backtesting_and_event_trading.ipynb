{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18412,
     "status": "ok",
     "timestamp": 1763199670568,
     "user": {
      "displayName": "Ayush Singh",
      "userId": "10589746899026829773"
     },
     "user_tz": -330
    },
    "id": "bqQgc6cyw2PU",
    "outputId": "c3180200-bd10-49f2-84a5-56aa58243cba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaning bad globals and restoring modules...\n",
      "Removed globals: []\n",
      "Globals now: np -> <class 'module'>  pd -> <class 'module'>  time -> <class 'module'>\n",
      "yfinance.utils._time OK: <class 'module'>\n",
      "\n",
      "Running smoke test for: ['^NSEI', 'AAPL', 'MSFT', 'RELIANCE.NS']\n",
      "^NSEI  -> rows: 1940 range: 2018-01-02 to 2025-11-14\n",
      "AAPL  -> rows: 1980 range: 2018-01-02 to 2025-11-14\n",
      "MSFT  -> rows: 1980 range: 2018-01-02 to 2025-11-14\n",
      "RELIANCE.NS  -> rows: 1945 range: 2018-01-01 to 2025-11-14\n",
      "[1/50] AAPL ... ok\n",
      "[2/50] MSFT ... ok\n",
      "[3/50] AMZN ... ok\n",
      "[4/50] GOOG ... ok\n",
      "[5/50] META ... ok\n",
      "[6/50] NVDA ... ok\n",
      "[7/50] TSLA ... ok\n",
      "[8/50] BRK-B ... ok\n",
      "[9/50] JPM ... ok\n",
      "[10/50] BAC ... ok\n",
      "[11/50] JNJ ... ok\n",
      "[12/50] V ... ok\n",
      "[13/50] MA ... ok\n",
      "[14/50] PG ... ok\n",
      "[15/50] KO ... ok\n",
      "[16/50] PFE ... ok\n",
      "[17/50] XOM ... ok\n",
      "[18/50] CVX ... ok\n",
      "[19/50] NFLX ... ok\n",
      "[20/50] DIS ... ok\n",
      "[21/50] RELIANCE.NS ... ok\n",
      "[22/50] TCS.NS ... ok\n",
      "[23/50] HDFCBANK.NS ... ok\n",
      "[24/50] ICICIBANK.NS ... ok\n",
      "[25/50] INFY.NS ... ok\n",
      "[26/50] LT.NS ... ok\n",
      "[27/50] BHARTIARTL.NS ... ok\n",
      "[28/50] ITC.NS ... ok\n",
      "[29/50] SUNPHARMA.NS ... ok\n",
      "[30/50] MARUTI.NS ... ok\n",
      "[31/50] AXISBANK.NS ... ok\n",
      "[32/50] HINDUNILVR.NS ... ok\n",
      "[33/50] EURUSD=X ... ok\n",
      "[34/50] GBPUSD=X ... ok\n",
      "[35/50] USDJPY=X ... ok\n",
      "[36/50] AUDUSD=X ... ok\n",
      "[37/50] USDCAD=X ... ok\n",
      "[38/50] USDCHF=X ... ok\n",
      "[39/50] NZDUSD=X ... ok\n",
      "[40/50] USDINR=X ... ok\n",
      "[41/50] GC=F ... ok\n",
      "[42/50] SI=F ... ok\n",
      "[43/50] CL=F ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipython-input-2586909172.py:198: RuntimeWarning: invalid value encountered in scalar power\n",
      "  summary_rows.append({\"ticker\":t,\"status\":\"ok\",\"rows\": len(s),\"cagr\": float((bt[\"equity\"].iloc[-1]) ** (1.0 / (len(bt)/252)) - 1.0) if len(bt)>1 else float(\"nan\"), \"sharpe\":sharpe,\"total_return\":total_return,\"max_drawdown\":maxdd})\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n",
      "[44/50] BZ=F ... ok\n",
      "[45/50] NG=F ... ok\n",
      "[46/50] ZC=F ... ok\n",
      "[47/50] SUGAR=F ... "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: SUGAR=F\"}}}\n",
      "ERROR:yfinance:$SUGAR=F: possibly delisted; no timezone found\n",
      "ERROR:yfinance:HTTP Error 404: {\"quoteSummary\":{\"result\":null,\"error\":{\"code\":\"Not Found\",\"description\":\"Quote not found for symbol: SUGAR=F\"}}}\n",
      "ERROR:yfinance:\n",
      "1 Failed download:\n",
      "ERROR:yfinance:['SUGAR=F']: YFTzMissingError('possibly delisted; no timezone found')\n",
      "ERROR:yfinance:$CATTLE=F: possibly delisted; no timezone found\n",
      "ERROR:yfinance:\n",
      "1 Failed download:\n",
      "ERROR:yfinance:['CATTLE=F']: YFTzMissingError('possibly delisted; no timezone found')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "skip (insufficient/missing)\n",
      "[48/50] CATTLE=F ... skip (insufficient/missing)\n",
      "[49/50] GLD ... ok\n",
      "[50/50] BTC-USD ... ok\n",
      "\n",
      "Batch finished. Summary saved to momentum_results/summary.csv\n",
      "      ticker status   rows      cagr    sharpe  total_return  max_drawdown\n",
      "     BTC-USD     ok 2876.0  0.263813  0.698731     13.470198     -0.744437\n",
      "        NVDA     ok 1980.0  0.238613  0.667569      4.372904     -0.651042\n",
      "         GLD     ok 1980.0  0.039074  0.294111      0.351430     -0.381760\n",
      "         JPM     ok 1980.0  0.029541  0.261673      0.257023     -0.757367\n",
      "        GC=F     ok 1981.0  0.026883  0.234066      0.231879     -0.414064\n",
      "    USDJPY=X     ok 2051.0  0.011119  0.163271      0.094172     -0.216332\n",
      "         PFE     ok 1980.0 -0.009341  0.087509     -0.071086     -0.509576\n",
      "SUNPHARMA.NS     ok 1945.0 -0.013002  0.079995     -0.096078     -0.490087\n",
      "      ITC.NS     ok 1945.0 -0.014802  0.091869     -0.108721     -0.617138\n",
      "       LT.NS     ok 1945.0 -0.017292  0.132520     -0.125962     -0.665907\n",
      "       BRK-B     ok 1980.0 -0.019835  0.035966     -0.145651     -0.499265\n",
      "    NZDUSD=X     ok 2052.0 -0.020732 -0.124167     -0.156835     -0.363489\n",
      "        TSLA     ok 1980.0 -0.027547  0.348389     -0.197063     -0.884635\n",
      "    EURUSD=X     ok 2051.0 -0.034375 -0.438290     -0.247754     -0.259886\n",
      "        AAPL     ok 1980.0 -0.034798  0.073014     -0.242918     -0.674921\n",
      "    GBPUSD=X     ok 2051.0 -0.034818 -0.343765     -0.250563     -0.299555\n",
      "    USDCHF=X     ok 2051.0 -0.035369 -0.372490     -0.254035     -0.306939\n",
      "    USDINR=X     ok 2051.0 -0.040874 -0.423488     -0.287985     -0.391952\n",
      "        MSFT     ok 1980.0 -0.049823 -0.056615     -0.330723     -0.738808\n",
      " RELIANCE.NS     ok 1945.0 -0.054567  0.061949     -0.351496     -0.727313\n"
     ]
    }
   ],
   "source": [
    "# Run this in the same session where you ran the diagnostic.\n",
    "# It will clean corrupted globals, patch yfinance, and run a small smoke test then the full batch.\n",
    "\n",
    "import importlib, sys, builtins, traceback, types, os, time as real_time\n",
    "print(\"Cleaning bad globals and restoring modules...\")\n",
    "\n",
    "# list of global names that diagnostics reported as strings\n",
    "bad_names = [\"time\",\"np\",\"pd\",\"download\",\"str\",\"start\",\"end\",\"sleep\"]\n",
    "\n",
    "g = globals()\n",
    "removed = []\n",
    "for name in bad_names:\n",
    "    if name in g:\n",
    "        val = g[name]\n",
    "        # Only remove if it's a str or otherwise suspicious\n",
    "        if isinstance(val, str) or isinstance(val, (types.FunctionType,)) and val.__class__ is str:\n",
    "            try:\n",
    "                del g[name]\n",
    "                removed.append(name)\n",
    "            except Exception as e:\n",
    "                print(\"Could not del global\", name, e)\n",
    "\n",
    "# restore common modules into globals (safe)\n",
    "import numpy as np, pandas as pd\n",
    "g[\"np\"] = np\n",
    "g[\"pd\"] = pd\n",
    "g[\"time\"] = real_time  # safe alias\n",
    "\n",
    "# restore builtins.str if somebody replaced it in globals by a string:\n",
    "# (we do NOT override builtins.str; we only remove a global named 'str' so builtin str() works)\n",
    "if \"str\" in g:\n",
    "    try:\n",
    "        del g[\"str\"]\n",
    "        removed.append(\"str\")\n",
    "    except Exception as e:\n",
    "        print(\"could not remove global 'str':\", e)\n",
    "\n",
    "print(\"Removed globals:\", removed)\n",
    "print(\"Globals now: np ->\", type(g.get(\"np\")), \" pd ->\", type(g.get(\"pd\")), \" time ->\", type(g.get(\"time\")))\n",
    "\n",
    "# patch yfinance internals (yfinance.utils._time must be real time)\n",
    "import yfinance as yf\n",
    "try:\n",
    "    import yfinance.utils as yutils\n",
    "    if getattr(yutils, \"_time\", None) is None or not callable(getattr(yutils, \"_time\", None)) and not hasattr(getattr(yutils, \"_time\", None), \"sleep\"):\n",
    "        yutils._time = real_time\n",
    "        print(\"Patched yfinance.utils._time -> real time module\")\n",
    "    else:\n",
    "        print(\"yfinance.utils._time OK:\", type(yutils._time))\n",
    "except Exception as e:\n",
    "    print(\"Could not patch yfinance.utils:\", e)\n",
    "    traceback.print_exc()\n",
    "\n",
    "# also patch yfinance.multi._time if present\n",
    "try:\n",
    "    import yfinance.multi as ym\n",
    "    if getattr(ym, \"_time\", None) is None or isinstance(getattr(ym, \"_time\", None), str):\n",
    "        ym._time = real_time\n",
    "        print(\"Patched yfinance.multi._time -> real time module\")\n",
    "except Exception:\n",
    "    pass\n",
    "\n",
    "# ---- robust fetch function (uses your Ticker.history first) ----\n",
    "import pandas as pd\n",
    "def fetch_close_series_like_user(ticker: str, start=None, end=None, max_retries=3):\n",
    "    \"\"\"Try yf.Ticker.history first (your working path), then fallback to yf.download.\n",
    "       Returns pd.Series (close/adj close) or None.\n",
    "    \"\"\"\n",
    "    for attempt in range(1, max_retries+1):\n",
    "        try:\n",
    "            df_hist = yf.Ticker(ticker).history(start=start, end=end, auto_adjust=True)\n",
    "            if df_hist is not None and not df_hist.empty:\n",
    "                # pick Close or Adj Close if present, else first column\n",
    "                if \"Close\" in df_hist.columns:\n",
    "                    s = df_hist[\"Close\"].astype(float)\n",
    "                elif \"Adj Close\" in df_hist.columns:\n",
    "                    s = df_hist[\"Adj Close\"].astype(float)\n",
    "                else:\n",
    "                    s = pd.Series(df_hist.iloc[:,0]).astype(float)\n",
    "                s = s.dropna()\n",
    "                s.index = pd.to_datetime(s.index)\n",
    "                if getattr(s.index, \"tz\", None) is not None:\n",
    "                    s.index = s.index.tz_localize(None)\n",
    "                return s.rename(ticker)\n",
    "            # fallback to download\n",
    "            df = yf.download(ticker, start=start, end=end, auto_adjust=True, progress=False)\n",
    "            if df is None or df.empty:\n",
    "                return None\n",
    "            col = \"Adj Close\" if \"Adj Close\" in df.columns else \"Close\"\n",
    "            s = df[col].astype(float).dropna()\n",
    "            s.index = pd.to_datetime(s.index)\n",
    "            if getattr(s.index, \"tz\", None) is not None:\n",
    "                s.index = s.index.tz_localize(None)\n",
    "            return s.rename(ticker)\n",
    "        except Exception as e:\n",
    "            msg = str(e)\n",
    "            # handle delisted/timezone errors gracefully\n",
    "            if \"YFTzMissingError\" in msg or \"Quote not found\" in msg or \"Not Found\" in msg:\n",
    "                print(f\"yfinance indicates missing/delisted for {ticker}: {msg}\")\n",
    "                return None\n",
    "            wait = min(5.0, 1.5 ** attempt)\n",
    "            print(f\"fetch attempt {attempt} failed for {ticker}: {e}  -- retrying in {wait:.1f}s\")\n",
    "            real_time.sleep(wait)\n",
    "    print(f\"error fetching {ticker} after {max_retries} attempts\")\n",
    "    return None\n",
    "\n",
    "# ---- quick smoke test using your fetch (4 tickers) ----\n",
    "test_tickers = [\"^NSEI\", \"AAPL\", \"MSFT\", \"RELIANCE.NS\"]\n",
    "print(\"\\nRunning smoke test for:\", test_tickers)\n",
    "smoke_ok = True\n",
    "for t in test_tickers:\n",
    "    try:\n",
    "        s = fetch_close_series_like_user(t, start=\"2018-01-01\", end=None)\n",
    "        if s is None:\n",
    "            print(t, \" -> NO DATA\")\n",
    "            smoke_ok = False\n",
    "        else:\n",
    "            print(t, \" -> rows:\", len(s), \"range:\", s.index[0].date(), \"to\", s.index[-1].date())\n",
    "    except Exception as e:\n",
    "        print(\"fetch error for\", t, e)\n",
    "        smoke_ok = False\n",
    "\n",
    "if not smoke_ok:\n",
    "    print(\"\\nSmoke test failed for one or more tickers. If this persists, do a runtime reset or paste the smoke output here.\")\n",
    "else:\n",
    "    # ---- full batch using your fetch function for all 50 tickers ----\n",
    "    TICKERS = [\n",
    "     \"AAPL\",\"MSFT\",\"AMZN\",\"GOOG\",\"META\",\"NVDA\",\"TSLA\",\"BRK-B\",\"JPM\",\"BAC\",\n",
    "     \"JNJ\",\"V\",\"MA\",\"PG\",\"KO\",\"PFE\",\"XOM\",\"CVX\",\"NFLX\",\"DIS\",\n",
    "     \"RELIANCE.NS\",\"TCS.NS\",\"HDFCBANK.NS\",\"ICICIBANK.NS\",\"INFY.NS\",\"LT.NS\",\n",
    "     \"BHARTIARTL.NS\",\"ITC.NS\",\"SUNPHARMA.NS\",\"MARUTI.NS\",\"AXISBANK.NS\",\"HINDUNILVR.NS\",\n",
    "     \"EURUSD=X\",\"GBPUSD=X\",\"USDJPY=X\",\"AUDUSD=X\",\"USDCAD=X\",\"USDCHF=X\",\"NZDUSD=X\",\"USDINR=X\",\n",
    "     \"GC=F\",\"SI=F\",\"CL=F\",\"BZ=F\",\"NG=F\",\"ZC=F\",\"SUGAR=F\",\"CATTLE=F\",\"GLD\",\"BTC-USD\"\n",
    "    ]\n",
    "    outdir = \"momentum_results\"\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    summary_rows = []\n",
    "    for i,t in enumerate(TICKERS, start=1):\n",
    "        print(f\"[{i}/{len(TICKERS)}] {t}\", end=\" ... \")\n",
    "        s = fetch_close_series_like_user(t, start=\"2018-01-01\", end=None)\n",
    "        if s is None or len(s) < 250:\n",
    "            print(\"skip (insufficient/missing)\")\n",
    "            summary_rows.append({\"ticker\":t,\"status\":\"skip\",\"rows\": None,\"cagr\": float(\"nan\")})\n",
    "            continue\n",
    "        # compute tanh signal (your compute_tanh_signal expects returns)\n",
    "        r = s.pct_change().fillna(0.0)\n",
    "        # simple tanh signal (lookback 63)\n",
    "        v = r.pow(2).ewm(span=63, adjust=False, min_periods=63).mean().pow(0.5)\n",
    "        u = r / (v + 1e-12)\n",
    "        m = u.ewm(span=63, adjust=False, min_periods=63).mean()\n",
    "        z = np.tanh(m)\n",
    "        std = z.dropna().std(ddof=0) if not z.dropna().empty else 1.0\n",
    "        sig = (z / std).fillna(0.0).shift(1)\n",
    "        # run your capital-sizing backtest (reusing backtest in your code)\n",
    "        # here replicate your backtest_capital_sizing directly:\n",
    "        idx = r.index\n",
    "        cap = float(100000.0)\n",
    "        prev_pos = 0.0\n",
    "        block = 0\n",
    "        in_trade = False\n",
    "        cap0 = cap\n",
    "        pos_list, cost_list, pnl_list, cap_list, ret_list = [], [], [], [], []\n",
    "        bps = 1.0 / 10000.0\n",
    "        for t_ix in range(len(idx)):\n",
    "            st = float(sig.iloc[t_ix]) if np.isfinite(sig.iloc[t_ix]) else 0.0\n",
    "            if block > 0:\n",
    "                pos = 0.0\n",
    "                block -= 1\n",
    "            else:\n",
    "                pos = st * cap\n",
    "            turn = abs(pos - prev_pos)\n",
    "            cost = bps * turn\n",
    "            rt = float(r.iloc[t_ix]) if np.isfinite(r.iloc[t_ix]) else 0.0\n",
    "            pnl = pos * rt\n",
    "            prev_cap = cap\n",
    "            cap = cap + pnl - cost\n",
    "            if pos != 0.0 and not in_trade:\n",
    "                in_trade = True\n",
    "                cap0 = cap\n",
    "            if in_trade:\n",
    "                trade_pnl = cap - cap0\n",
    "                if trade_pnl <= -0.01 * cap0 or trade_pnl >= 0.06 * cap0:\n",
    "                    block = 1\n",
    "                    in_trade = False\n",
    "            pos_list.append(pos); cost_list.append(cost); pnl_list.append(pnl); cap_list.append(cap)\n",
    "            ret_list.append((pnl - cost) / max(prev_cap, 1e-12))\n",
    "            prev_pos = pos\n",
    "        bt = pd.DataFrame({\"position_inr\":pd.Series(pos_list,index=idx),\"pnl_inr\":pd.Series(pnl_list,index=idx),\"cost_inr\":pd.Series(cost_list,index=idx),\"capital_inr\":pd.Series(cap_list,index=idx),\"strategy_ret\":pd.Series(ret_list,index=idx)})\n",
    "        bt[\"equity\"] = bt[\"capital_inr\"] / 100000.0\n",
    "        # compute metrics\n",
    "        total_return = bt[\"equity\"].iloc[-1] - 1.0\n",
    "        ann_ret = bt[\"strategy_ret\"].mean() * 252\n",
    "        ann_vol = bt[\"strategy_ret\"].std(ddof=1) * (252**0.5) if len(bt) > 1 else float(\"nan\")\n",
    "        sharpe = ann_ret / ann_vol if ann_vol and ann_vol > 0 else float(\"nan\")\n",
    "        cum = bt[\"equity\"].cummax()\n",
    "        drawdown = bt[\"equity\"] / cum - 1.0\n",
    "        maxdd = drawdown.min()\n",
    "        summary_rows.append({\"ticker\":t,\"status\":\"ok\",\"rows\": len(s),\"cagr\": float((bt[\"equity\"].iloc[-1]) ** (1.0 / (len(bt)/252)) - 1.0) if len(bt)>1 else float(\"nan\"), \"sharpe\":sharpe,\"total_return\":total_return,\"max_drawdown\":maxdd})\n",
    "        bt.to_csv(os.path.join(outdir, f\"{t.replace('/','_')}_backtest.csv\"))\n",
    "        print(\"ok\")\n",
    "    # save summary\n",
    "    import pandas as pd\n",
    "    summary_df = pd.DataFrame(summary_rows).sort_values(\"cagr\", ascending=False, na_position=\"last\").reset_index(drop=True)\n",
    "    summary_df.to_csv(os.path.join(outdir,\"summary.csv\"), index=False)\n",
    "    print(\"\\nBatch finished. Summary saved to\", os.path.join(outdir,\"summary.csv\"))\n",
    "    print(summary_df.head(20).to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 41265,
     "status": "ok",
     "timestamp": 1763199715087,
     "user": {
      "displayName": "Ayush Singh",
      "userId": "10589746899026829773"
     },
     "user_tz": -330
    },
    "id": "_wskrhKRzC12",
    "outputId": "6408bb27-ec27-44ea-8706-31c7d8f8ed4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top winners: ['BTC-USD', 'NVDA', 'GLD', 'JPM', 'GC=F', 'USDJPY=X']\n",
      "Top losers:  ['CVX', 'MA', 'NFLX', 'BAC', 'HDFCBANK.NS', 'NG=F']\n",
      "Wrote momentum_results/analysis_summary.csv\n",
      "\n",
      "Top 10 by CAGR (summary.csv):\n",
      "      ticker status   rows      cagr   sharpe  total_return  max_drawdown\n",
      "     BTC-USD     ok 2876.0  0.263813 0.698731     13.470198     -0.744437\n",
      "        NVDA     ok 1980.0  0.238613 0.667569      4.372904     -0.651042\n",
      "         GLD     ok 1980.0  0.039074 0.294111      0.351430     -0.381760\n",
      "         JPM     ok 1980.0  0.029541 0.261673      0.257023     -0.757367\n",
      "        GC=F     ok 1981.0  0.026883 0.234066      0.231879     -0.414064\n",
      "    USDJPY=X     ok 2051.0  0.011119 0.163271      0.094172     -0.216332\n",
      "         PFE     ok 1980.0 -0.009341 0.087509     -0.071086     -0.509576\n",
      "SUNPHARMA.NS     ok 1945.0 -0.013002 0.079995     -0.096078     -0.490087\n",
      "      ITC.NS     ok 1945.0 -0.014802 0.091869     -0.108721     -0.617138\n",
      "       LT.NS     ok 1945.0 -0.017292 0.132520     -0.125962     -0.665907\n",
      "\n",
      "Analysis head (analysis_summary.csv):\n",
      "      ticker status   rows  n_trades  win_rate  avg_trade  median_trade  avg_hold_bars  total_return                cagr   ann_ret  ann_vol    sharpe  bootstrap_pval  max_drawdown\n",
      "     BTC-USD     ok 2876.0    2349.0  0.491273   0.001952     -0.000041            1.0     13.470198  0.263813+0.000000j  0.397880 0.569432  0.698731          0.4970     -0.744437\n",
      "        NVDA     ok 1980.0    1518.0  0.525033   0.001818      0.000685            1.0      4.372904  0.238613+0.000000j  0.346069 0.518402  0.667569          0.4925     -0.651042\n",
      "         GLD     ok 1980.0    1738.0  0.526467   0.000278      0.000084            1.0      0.351430  0.039074+0.000000j  0.059493 0.202280  0.294111          0.5705     -0.381760\n",
      "         JPM     ok 1980.0    1660.0  0.516867   0.000468      0.000124            1.0      0.257023  0.029541+0.000000j  0.095963 0.366728  0.261673          0.5715     -0.757367\n",
      "        GC=F     ok 1981.0    1724.0  0.516241   0.000235      0.000083            1.0      0.231879  0.026883+0.000000j  0.049127 0.209884  0.234066          0.5865     -0.414064\n",
      "    USDJPY=X     ok 2051.0    1871.0  0.509353   0.000072      0.000023            1.0      0.094172  0.011119+0.000000j  0.015621 0.095674  0.163271          0.6655     -0.216332\n",
      "         PFE     ok 1980.0    1680.0  0.484524   0.000115     -0.000046            1.0     -0.071086 -0.009341+0.000000j  0.021936 0.250668  0.087509          0.8265     -0.509576\n",
      "SUNPHARMA.NS     ok 1945.0    1623.0  0.502157   0.000113      0.000013            1.0     -0.096078 -0.013002+0.000000j  0.020947 0.261854  0.079995          0.8300     -0.490087\n",
      "      ITC.NS     ok 1945.0    1650.0  0.487879   0.000135     -0.000058            1.0     -0.108721 -0.014802+0.000000j  0.026235 0.285567  0.091869          0.7970     -0.617138\n",
      "       LT.NS     ok 1945.0    1649.0  0.509400   0.000240      0.000047            1.0     -0.125962 -0.017292+0.000000j  0.048503 0.366005  0.132520          0.7370     -0.665907\n",
      "       BRK-B     ok 1980.0    1701.0  0.519106   0.000051      0.000123            1.0     -0.145651 -0.019835+0.000000j  0.008469 0.235477  0.035966          0.9245     -0.499265\n",
      "    NZDUSD=X     ok 2052.0    1849.0  0.500811  -0.000058      0.000002            1.0     -0.156835 -0.020732+0.000000j -0.014311 0.115257 -0.124167          0.7395     -0.363489\n",
      "\n",
      "Plots saved to momentum_results/analysis_plots\n"
     ]
    }
   ],
   "source": [
    "# Paste & run in the notebook that already has momentum_results/ created\n",
    "import os, math, json, traceback\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "ROOT = Path(\"momentum_results\")\n",
    "OUTP = ROOT / \"analysis_plots\"\n",
    "OUTP.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# load summary\n",
    "summary_path = ROOT / \"summary.csv\"\n",
    "if not summary_path.exists():\n",
    "    raise FileNotFoundError(\"momentum_results/summary.csv not found. Run batch first.\")\n",
    "summary = pd.read_csv(summary_path)\n",
    "summary = summary.sort_values(\"cagr\", ascending=False).reset_index(drop=True)\n",
    "\n",
    "# helper: load backtest for a ticker\n",
    "def load_bt(ticker):\n",
    "    fname = ROOT / f\"{ticker.replace('/','_')}_backtest.csv\"\n",
    "    if not fname.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(fname, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "# extract trades function (reproduced for clarity)\n",
    "def extract_trades_df(pos, strat_ret):\n",
    "    p = pos.fillna(0.0).astype(float)\n",
    "    sr = strat_ret.fillna(0.0).astype(float)\n",
    "    if p.empty:\n",
    "        return pd.DataFrame(columns=[\"start\",\"end\",\"side\",\"bars\",\"trade_return\"])\n",
    "    ch = p.ne(p.shift(1)).fillna(p.ne(0.0))\n",
    "    idx = p.index\n",
    "    cps = list(np.where(ch.values)[0])\n",
    "    if cps and cps[0] != 0:\n",
    "        cps = [0] + cps\n",
    "    if not cps:\n",
    "        cps = [0]\n",
    "    cps.append(len(p))\n",
    "    trades=[]\n",
    "    for i in range(len(cps)-1):\n",
    "        s=cps[i]; e=cps[i+1]\n",
    "        v=float(p.iloc[s])\n",
    "        if v==0.0:\n",
    "            continue\n",
    "        period_idx = idx[s:e]\n",
    "        tr = float((1.0 + sr.loc[period_idx]).prod() - 1.0)\n",
    "        trades.append({\"start\": period_idx[0], \"end\": period_idx[-1], \"side\": \"long\" if v>0 else \"short\", \"bars\": int(len(period_idx)), \"trade_return\": tr})\n",
    "    return pd.DataFrame(trades)\n",
    "\n",
    "# bootstrap p-value helper\n",
    "def bootstrap_pvalue(series, iters=2000, seed=42):\n",
    "    arr = np.array(series.dropna())\n",
    "    if arr.size == 0:\n",
    "        return np.nan\n",
    "    rng = np.random.RandomState(seed)\n",
    "    obs = arr.mean()\n",
    "    means = []\n",
    "    for _ in range(iters):\n",
    "        sample = rng.choice(arr, size=arr.size, replace=True)\n",
    "        means.append(sample.mean())\n",
    "    means = np.array(means)\n",
    "    pval = (np.abs(means) >= np.abs(obs)).mean()\n",
    "    return float(pval)\n",
    "\n",
    "# analysis per ticker\n",
    "analysis_rows = []\n",
    "top_n = 6\n",
    "winners = summary.dropna(subset=[\"cagr\"]).nlargest(top_n, \"cagr\")[\"ticker\"].tolist()\n",
    "losers = summary.dropna(subset=[\"cagr\"]).nsmallest(top_n, \"cagr\")[\"ticker\"].tolist()\n",
    "\n",
    "print(\"Top winners:\", winners)\n",
    "print(\"Top losers: \", losers)\n",
    "\n",
    "for row in summary.itertuples(index=False):\n",
    "    t = row.ticker\n",
    "    bt = load_bt(t)\n",
    "    if bt is None:\n",
    "        analysis_rows.append({\"ticker\":t, \"status\": \"missing\"})\n",
    "        continue\n",
    "    # trades\n",
    "    trades = extract_trades_df(bt[\"position_inr\"], bt[\"strategy_ret\"])\n",
    "    n_trades = 0 if trades.empty else len(trades)\n",
    "    win_rate = np.nan if trades.empty else (trades[\"trade_return\"] > 0).mean()\n",
    "    avg_trade = np.nan if trades.empty else trades[\"trade_return\"].mean()\n",
    "    median_trade = np.nan if trades.empty else trades[\"trade_return\"].median()\n",
    "    avg_hold = np.nan if trades.empty else trades[\"bars\"].mean()\n",
    "    # annualized stats (recompute to be safe)\n",
    "    equity = bt[\"equity\"]\n",
    "    strat = bt[\"strategy_ret\"]\n",
    "    total_return = float(equity.iloc[-1] - 1.0)\n",
    "    years = len(strat) / 252.0\n",
    "    cagr = (float(equity.iloc[-1]) ** (1.0 / years) - 1.0) if years>0 else np.nan\n",
    "    ann_ret = float(strat.mean() * 252.0)\n",
    "    ann_vol = float(strat.std(ddof=1) * math.sqrt(252.0)) if len(strat)>1 else np.nan\n",
    "    sharpe = ann_ret / ann_vol if ann_vol and ann_vol>0 else np.nan\n",
    "    # bootstrap p-value for mean strategy return\n",
    "    pval = bootstrap_pvalue(strat, iters=2000, seed=123)\n",
    "    analysis_rows.append({\n",
    "        \"ticker\": t,\n",
    "        \"status\": \"ok\",\n",
    "        \"rows\": len(bt),\n",
    "        \"n_trades\": n_trades,\n",
    "        \"win_rate\": win_rate,\n",
    "        \"avg_trade\": avg_trade,\n",
    "        \"median_trade\": median_trade,\n",
    "        \"avg_hold_bars\": avg_hold,\n",
    "        \"total_return\": total_return,\n",
    "        \"cagr\": cagr,\n",
    "        \"ann_ret\": ann_ret,\n",
    "        \"ann_vol\": ann_vol,\n",
    "        \"sharpe\": sharpe,\n",
    "        \"bootstrap_pval\": pval,\n",
    "        \"max_drawdown\": row.max_drawdown if \"max_drawdown\" in row._fields else np.nan\n",
    "    })\n",
    "\n",
    "# save analysis table\n",
    "analysis_df = pd.DataFrame(analysis_rows)\n",
    "analysis_df.to_csv(ROOT / \"analysis_summary.csv\", index=False)\n",
    "print(\"Wrote momentum_results/analysis_summary.csv\")\n",
    "\n",
    "# plot top winners/losers equity curves and trade return histograms\n",
    "def plot_equity_and_trades(ticker, bt):\n",
    "    fig, ax = plt.subplots(1,2, figsize=(12,3))\n",
    "    bt[\"equity\"].plot(ax=ax[0], title=f\"{ticker} equity (final {bt['equity'].iloc[-1]:.3f})\")\n",
    "    ax[0].set_ylabel(\"Equity (rebased)\")\n",
    "    trades = extract_trades_df(bt[\"position_inr\"], bt[\"strategy_ret\"])\n",
    "    if not trades.empty:\n",
    "        ax[1].hist(trades[\"trade_return\"].dropna(), bins=40)\n",
    "        ax[1].set_title(f\"{ticker} trade returns (n={len(trades)})\")\n",
    "        ax[1].set_xlabel(\"trade return\")\n",
    "    else:\n",
    "        ax[1].text(0.5,0.5,\"no trades\", ha=\"center\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(OUTP / f\"{ticker}_eq_trades.png\")\n",
    "    plt.close()\n",
    "\n",
    "for t in winners + losers:\n",
    "    bt = load_bt(t)\n",
    "    if bt is not None:\n",
    "        plot_equity_and_trades(t, bt)\n",
    "\n",
    "# summary prints\n",
    "print(\"\\nTop 10 by CAGR (summary.csv):\")\n",
    "print(summary.head(10).to_string(index=False))\n",
    "print(\"\\nAnalysis head (analysis_summary.csv):\")\n",
    "print(analysis_df.sort_values(\"cagr\", ascending=False).head(12).to_string(index=False))\n",
    "\n",
    "print(\"\\nPlots saved to\", OUTP)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kU2r1eAb1qOc",
    "outputId": "130520b1-db1f-453f-fddf-117678ce102e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected tickers for figures: ['BTC-USD', 'NVDA', 'GLD', 'CVX', 'MA', 'NFLX']\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "make_report_figs.py\n",
    "\n",
    "Load per-ticker backtests from momentum_results/ and create a small set\n",
    "of publication-quality figures for the LaTeX report. Also write a LaTeX\n",
    "snippet file (figures.tex) that includes the generated PNGs.\n",
    "\n",
    "Usage:\n",
    "    python make_report_figs.py\n",
    "\n",
    "Dependencies:\n",
    "    pip install pandas numpy matplotlib\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---- user settings ----\n",
    "RESULTS_DIR = Path(\"momentum_results\")\n",
    "OUT_DIR = Path(\"report_figs\")\n",
    "OUT_DIR.mkdir(exist_ok=True)\n",
    "SUMMARY_CSV = RESULTS_DIR / \"summary.csv\"\n",
    "TOP_N = 3   # top 3 winners\n",
    "BOTTOM_N = 3  # top 3 losers\n",
    "FIG_DPI = 200\n",
    "\n",
    "# ---- helpers ----\n",
    "def load_backtest(ticker: str):\n",
    "    fn = RESULTS_DIR / f\"{ticker.replace('/','_')}_backtest.csv\"\n",
    "    if not fn.exists():\n",
    "        return None\n",
    "    df = pd.read_csv(fn, index_col=0, parse_dates=True)\n",
    "    return df\n",
    "\n",
    "def extract_trades(pos, strat_ret):\n",
    "    p = pos.fillna(0.0).astype(float)\n",
    "    sr = strat_ret.fillna(0.0).astype(float)\n",
    "    if p.empty:\n",
    "        return pd.DataFrame(columns=[\"start\",\"end\",\"side\",\"bars\",\"trade_return\"])\n",
    "    ch = p.ne(p.shift(1)).fillna(p.ne(0.0))\n",
    "    idx = p.index\n",
    "    cps = list(np.where(ch.values)[0])\n",
    "    if cps and cps[0] != 0:\n",
    "        cps = [0] + cps\n",
    "    if not cps:\n",
    "        cps = [0]\n",
    "    cps.append(len(p))\n",
    "    trades = []\n",
    "    for i in range(len(cps)-1):\n",
    "        s = cps[i]; e = cps[i+1]\n",
    "        v = float(p.iloc[s])\n",
    "        if v == 0.0:\n",
    "            continue\n",
    "        period_idx = idx[s:e]\n",
    "        tr = float((1.0 + sr.loc[period_idx]).prod() - 1.0)\n",
    "        trades.append({\"start\": period_idx[0], \"end\": period_idx[-1], \"side\": \"long\" if v>0 else \"short\", \"bars\": int(len(period_idx)), \"trade_return\": tr})\n",
    "    return pd.DataFrame(trades)\n",
    "\n",
    "def safe_plot_save(fig, path):\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(path, dpi=FIG_DPI)\n",
    "    plt.close(fig)\n",
    "\n",
    "# ---- load summary and select tickers ----\n",
    "if not SUMMARY_CSV.exists():\n",
    "    raise FileNotFoundError(f\"summary.csv not found in {RESULTS_DIR}. Run batch first.\")\n",
    "\n",
    "summary = pd.read_csv(SUMMARY_CSV)\n",
    "# ensure cagr numeric\n",
    "summary['cagr'] = pd.to_numeric(summary['cagr'], errors='coerce')\n",
    "top = summary.dropna(subset=['cagr']).nlargest(TOP_N, 'cagr')['ticker'].tolist()\n",
    "bottom = summary.dropna(subset=['cagr']).nsmallest(BOTTOM_N, 'cagr')['ticker'].tolist()\n",
    "\n",
    "# fallback if not enough\n",
    "selected = []\n",
    "for t in top:\n",
    "    selected.append(t)\n",
    "for t in bottom:\n",
    "    if t not in selected:\n",
    "        selected.append(t)\n",
    "\n",
    "print(\"Selected tickers for figures:\", selected)\n",
    "\n",
    "# ---- 1) Combined equity curves for top N winners ----\n",
    "def plot_top_equities(tickers, outpath):\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    for t in tickers:\n",
    "        bt = load_backtest(t)\n",
    "        if bt is None:\n",
    "            print(f\"warning: backtest file missing for {t}\")\n",
    "            continue\n",
    "        ax.plot(bt.index, bt['equity'], label=f\"{t} (final {bt['equity'].iloc[-1]:.2f})\", linewidth=1.2)\n",
    "    ax.set_title(\"Top %d Tickers — Equity Curves\" % len(tickers))\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Equity (relative)\")\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "plot_top_equities(top, OUT_DIR / \"01_top3_equity.png\")\n",
    "\n",
    "# ---- 2) Drawdown plot for top N ----\n",
    "def drawdown(series):\n",
    "    cm = series.cummax()\n",
    "    return series / cm - 1.0\n",
    "\n",
    "def plot_top_drawdowns(tickers, outpath):\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    for t in tickers:\n",
    "        bt = load_backtest(t)\n",
    "        if bt is None: continue\n",
    "        dd = drawdown(bt['equity'])\n",
    "        ax.plot(dd.index, dd, label=t, linewidth=1.2)\n",
    "    ax.set_title(\"Top %d Tickers — Drawdowns\" % len(tickers))\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Drawdown\")\n",
    "    ax.axhline(0, color='k', linewidth=0.6)\n",
    "    ax.legend(loc='best', fontsize=8)\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "plot_top_drawdowns(top, OUT_DIR / \"02_top3_drawdown.png\")\n",
    "\n",
    "# ---- 3) Trade-return histograms for top N (subplot grid) ----\n",
    "def plot_trade_histograms(tickers, outpath):\n",
    "    n = len(tickers)\n",
    "    cols = min(3, n)\n",
    "    rows = math.ceil(n / cols)\n",
    "    fig, axes = plt.subplots(rows, cols, figsize=(cols*4, rows*3))\n",
    "    if isinstance(axes, np.ndarray):\n",
    "        axes = axes.flatten()\n",
    "    else:\n",
    "        axes = [axes]\n",
    "    for i, t in enumerate(tickers):\n",
    "        bt = load_backtest(t)\n",
    "        if bt is None:\n",
    "            axes[i].text(0.5,0.5,\"missing\", ha='center')\n",
    "            continue\n",
    "        trades = extract_trades(bt['position_inr'], bt['strategy_ret'])\n",
    "        if trades.empty:\n",
    "            axes[i].text(0.5,0.5,\"no trades\", ha='center')\n",
    "            continue\n",
    "        axes[i].hist(trades['trade_return'].dropna(), bins=40)\n",
    "        axes[i].set_title(t)\n",
    "        axes[i].set_xlabel(\"trade return\"); axes[i].set_ylabel(\"count\")\n",
    "    # hide unused axes\n",
    "    for j in range(n, rows*cols):\n",
    "        axes[j].axis('off')\n",
    "    fig.suptitle(\"Trade Return Distributions (Top tickers)\")\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "plot_trade_histograms(top, OUT_DIR / \"03_top3_trade_hist.png\")\n",
    "\n",
    "# ---- 4) Equal-weight portfolio equity of top N (one curve) ----\n",
    "def plot_portfolio_equity(tickers, outpath):\n",
    "    dfs = []\n",
    "    for t in tickers:\n",
    "        bt = load_backtest(t)\n",
    "        if bt is None: continue\n",
    "        dfs.append(bt['equity'].rename(t))\n",
    "    if not dfs:\n",
    "        print(\"no backtests for portfolio plot\")\n",
    "        return\n",
    "    eq_all = pd.concat(dfs, axis=1).dropna(how='all')\n",
    "    # rebase each to 1 at first common date\n",
    "    eq_all = eq_all / eq_all.iloc[0]\n",
    "    # equal-weight portfolio (simple mean)\n",
    "    port = eq_all.mean(axis=1)\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    ax.plot(port.index, port.values, linewidth=1.4)\n",
    "    ax.set_title(\"Equal-weight Portfolio Equity (Top %d)\" % len(tickers))\n",
    "    ax.set_xlabel(\"Date\"); ax.set_ylabel(\"Equity (relative)\")\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "plot_portfolio_equity(top, OUT_DIR / \"04_portfolio_equity.png\")\n",
    "\n",
    "# ---- 5) BTC price vs tanh-signal overlay (if BTC present) ----\n",
    "def plot_price_vs_signal(ticker, outpath):\n",
    "    bt = load_backtest(ticker)\n",
    "    if bt is None:\n",
    "        print(f\"no backtest for {ticker}\")\n",
    "        return\n",
    "    # try to locate price and reconstruct signal approximately from position sign\n",
    "    # We will attempt to load original price from the backtest CSV if present as 'capital_inr' or recreate placeholder\n",
    "    # If actual signal series file isn't available, we plot equity and position sign as proxy.\n",
    "    fig, ax1 = plt.subplots(figsize=(8,4))\n",
    "    # plot equity as proxy for price movement interest\n",
    "    ax1.plot(bt.index, bt['capital_inr'], label='Capital (INR)', color='black')\n",
    "    ax1.set_ylabel(\"Capital (INR)\", color='black')\n",
    "    ax2 = ax1.twinx()\n",
    "    pos_sign = np.sign(bt['position_inr'].fillna(0.0))\n",
    "    ax2.plot(bt.index, pos_sign, label='Position sign', color='orange', alpha=0.7)\n",
    "    ax2.set_ylabel(\"Position sign\", color='orange')\n",
    "    ax1.set_title(f\"{ticker}: Capital and Position Sign (proxy for signal)\")\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "# prefer BTC-USD if present, else first top ticker\n",
    "btc = \"BTC-USD\" if \"BTC-USD\" in summary['ticker'].values else (top[0] if top else None)\n",
    "if btc:\n",
    "    plot_price_vs_signal(btc, OUT_DIR / \"05_btc_signal_price.png\")\n",
    "\n",
    "# ---- 6) Correlation heatmap of returns for selected tickers (top 3 winners + bottom 3 losers) ----\n",
    "def plot_corr_heatmap(tickers, outpath):\n",
    "    rets = []\n",
    "    names = []\n",
    "    for t in tickers:\n",
    "        bt = load_backtest(t)\n",
    "        if bt is None: continue\n",
    "        # attempt daily returns from backtest; fallback to diff of equity\n",
    "        if 'strategy_ret' in bt.columns:\n",
    "            r = bt['strategy_ret']\n",
    "        else:\n",
    "            r = bt['equity'].pct_change().fillna(0.0)\n",
    "        rets.append(r.rename(t))\n",
    "        names.append(t)\n",
    "    if not rets:\n",
    "        print(\"no returns for correlation heatmap\")\n",
    "        return\n",
    "    dfR = pd.concat(rets, axis=1).dropna(how='all')\n",
    "    corr = dfR.corr()\n",
    "    fig, ax = plt.subplots(figsize=(6,5))\n",
    "    cax = ax.matshow(corr, cmap='RdBu_r', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(range(len(names))); ax.set_yticks(range(len(names)))\n",
    "    ax.set_xticklabels(names, rotation=45, ha='left')\n",
    "    ax.set_yticklabels(names)\n",
    "    fig.colorbar(cax, fraction=0.025)\n",
    "    ax.set_title(\"Return Correlation (selected tickers)\")\n",
    "    safe_plot_save(fig, outpath)\n",
    "\n",
    "plot_corr_heatmap(selected, OUT_DIR / \"06_corr_heatmap_top6.png\")\n",
    "\n",
    "# ---- write LaTeX snippet to include figures ----\n",
    "tex_path = OUT_DIR / \"figures.tex\"\n",
    "with open(tex_path, \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"% Auto-generated LaTeX snippet including the report figures\\n\")\n",
    "    f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"01_top3_equity.png\") + \"}\\n\")\n",
    "    f.write(\"\\\\caption{Equity curves — top 3 tickers by CAGR.}\\n\\\\end{figure}\\n\\n\")\n",
    "    f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"02_top3_drawdown.png\") + \"}\\n\")\n",
    "    f.write(\"\\\\caption{Drawdowns — top 3 tickers.}\\n\\\\end{figure}\\n\\n\")\n",
    "    f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"03_top3_trade_hist.png\") + \"}\\n\")\n",
    "    f.write(\"\\\\caption{Trade return distributions for top tickers.}\\n\\\\end{figure}\\n\\n\")\n",
    "    f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"04_portfolio_equity.png\") + \"}\\n\")\n",
    "    f.write(\"\\\\caption{Equal-weight portfolio equity of top 3 tickers.}\\n\\\\end{figure}\\n\\n\")\n",
    "    if btc:\n",
    "        f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "        f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"05_btc_signal_price.png\") + \"}\\n\")\n",
    "        f.write(\"\\\\caption{BTC-USD: capital and position sign (signal proxy).}\\n\\\\end{figure}\\n\\n\")\n",
    "    f.write(\"\\\\begin{figure}[htbp]\\n\\\\centering\\n\")\n",
    "    f.write(\"\\\\includegraphics[width=0.95\\\\linewidth]{\" + str(OUT_DIR / \"06_corr_heatmap_top6.png\") + \"}\\n\")\n",
    "    f.write(\"\\\\caption{Return correlation between selected tickers (top winners + losers).}\\n\\\\end{figure}\\n\")\n",
    "print(\"Figures written to\", OUT_DIR)\n",
    "print(\"LaTeX snippet written to\", tex_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65604,
     "status": "ok",
     "timestamp": 1763200175071,
     "user": {
      "displayName": "Ayush Singh",
      "userId": "10589746899026829773"
     },
     "user_tz": -330
    },
    "id": "zAlD-QyT3jnW",
    "outputId": "1472197c-f04a-4038-e1b1-2e5a8727ab73"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48 backtest files. Running event analysis...\n",
      "\n",
      "Processing AAPL ...\n",
      "\n",
      "Processing AMZN ...\n",
      "\n",
      "Processing AUDUSD=X ...\n",
      "\n",
      "Processing AXISBANK.NS ...\n",
      "\n",
      "Processing BAC ...\n",
      "\n",
      "Processing BHARTIARTL.NS ...\n",
      "\n",
      "Processing BRK-B ...\n",
      "\n",
      "Processing BTC-USD ...\n",
      "\n",
      "Processing BZ=F ...\n",
      "\n",
      "Processing CL=F ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/numpy/_core/_methods.py:56: RuntimeWarning: overflow encountered in reduce\n",
      "  return umr_prod(a, axis, dtype, out, keepdims, initial, where)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing CVX ...\n",
      "\n",
      "Processing DIS ...\n",
      "\n",
      "Processing EURUSD=X ...\n",
      "\n",
      "Processing GBPUSD=X ...\n",
      "\n",
      "Processing GC=F ...\n",
      "\n",
      "Processing GLD ...\n",
      "\n",
      "Processing GOOG ...\n",
      "\n",
      "Processing HDFCBANK.NS ...\n",
      "\n",
      "Processing HINDUNILVR.NS ...\n",
      "\n",
      "Processing ICICIBANK.NS ...\n",
      "\n",
      "Processing INFY.NS ...\n",
      "\n",
      "Processing ITC.NS ...\n",
      "\n",
      "Processing JNJ ...\n",
      "\n",
      "Processing JPM ...\n",
      "\n",
      "Processing KO ...\n",
      "\n",
      "Processing LT.NS ...\n",
      "\n",
      "Processing MARUTI.NS ...\n",
      "\n",
      "Processing MA ...\n",
      "\n",
      "Processing META ...\n",
      "\n",
      "Processing MSFT ...\n",
      "\n",
      "Processing NFLX ...\n",
      "\n",
      "Processing NG=F ...\n",
      "\n",
      "Processing NVDA ...\n",
      "\n",
      "Processing NZDUSD=X ...\n",
      "\n",
      "Processing PFE ...\n",
      "\n",
      "Processing PG ...\n",
      "\n",
      "Processing RELIANCE.NS ...\n",
      "\n",
      "Processing SI=F ...\n",
      "\n",
      "Processing SUNPHARMA.NS ...\n",
      "\n",
      "Processing TCS.NS ...\n",
      "\n",
      "Processing TSLA ...\n",
      "\n",
      "Processing USDCAD=X ...\n",
      "\n",
      "Processing USDCHF=X ...\n",
      "\n",
      "Processing USDINR=X ...\n",
      "\n",
      "Processing USDJPY=X ...\n",
      "\n",
      "Processing V ...\n",
      "\n",
      "Processing XOM ...\n",
      "\n",
      "Processing ZC=F ...\n",
      "\n",
      "Event analysis summary written to: momentum_results/event_analysis_summary.csv\n",
      "\n",
      "Done. Check the folder 'event_figs/' for plots and the CSV summary file.\n"
     ]
    }
   ],
   "source": [
    "# event_analysis_batch.py\n",
    "\"\"\"\n",
    "Event-based analysis for momentum backtests.\n",
    "\n",
    "Expect per-ticker CSVs in: momentum_results/\n",
    "Files should be named like: <TICKER>_backtest.csv (or similar that endswith '_backtest.csv')\n",
    "\n",
    "Output:\n",
    " - event_figs/<TICKER>/<EVENT_NAME>.png  (plots)\n",
    " - momentum_results/event_analysis_summary.csv (summary table)\n",
    "\n",
    "Usage:\n",
    "    python event_analysis_batch.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "\n",
    "# --------- CONFIG ---------\n",
    "BACKTEST_FOLDER = \"momentum_results\"\n",
    "OUT_FIG_FOLDER = \"event_figs\"\n",
    "SUMMARY_CSV = os.path.join(BACKTEST_FOLDER, \"event_analysis_summary.csv\")\n",
    "\n",
    "# Define event windows (you can edit/add)\n",
    "EVENTS = {\n",
    "    \"COVID_Crash_2020\": (\"2020-02-01\", \"2020-04-01\"),\n",
    "    \"US_Elections_2020\": (\"2020-10-01\", \"2020-12-15\"),\n",
    "    \"Russia_Ukraine_2022\": (\"2022-02-01\", \"2022-03-15\"),\n",
    "    \"Crypto_Crash_2021\": (\"2021-05-01\", \"2021-07-01\"),\n",
    "    \"FTX_Collapse_2022\": (\"2022-10-01\", \"2022-12-15\"),\n",
    "}\n",
    "\n",
    "# Pre/post sample size for simple diagnostics\n",
    "PRE_DAYS = 20\n",
    "POST_DAYS = 20\n",
    "\n",
    "# Minimal sanity checks\n",
    "MIN_DAYS_REQUIRED = 10   # skip too-short series\n",
    "\n",
    "# --------- HELPERS ----------\n",
    "def safe_read_backtest(path):\n",
    "    \"\"\"\n",
    "    Read a backtest CSV and return a DataFrame with expected columns:\n",
    "      - datetime index\n",
    "      - position_inr\n",
    "      - strategy_ret\n",
    "      - equity (if missing, compute from capital_inr / first capital)\n",
    "    Returns DataFrame or None if file not usable.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(path, parse_dates=True, index_col=0)\n",
    "    except Exception as e:\n",
    "        print(f\"FAILED read {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "    # ensure datetime index\n",
    "    if not isinstance(df.index, pd.DatetimeIndex):\n",
    "        try:\n",
    "            df.index = pd.to_datetime(df.index)\n",
    "        except Exception:\n",
    "            # if first column is a date\n",
    "            df = df.reset_index()\n",
    "            df.iloc[:,0] = pd.to_datetime(df.iloc[:,0])\n",
    "            df = df.set_index(df.columns[0])\n",
    "\n",
    "    cols = df.columns.str.lower()\n",
    "    # unify column names by lowercase variants\n",
    "    df.columns = [c.lower() for c in df.columns]\n",
    "\n",
    "    # require 'strategy_ret' or 'strategy_ret' like\n",
    "    if \"strategy_ret\" not in df.columns and \"strategy return\" not in df.columns:\n",
    "        # maybe there is 'strategy_ret' spelled differently: try \"strategy_ret\"\n",
    "        if \"strategy\" in df.columns:\n",
    "            df[\"strategy_ret\"] = df[\"strategy\"]\n",
    "        else:\n",
    "            print(f\"File {path} missing 'strategy_ret' column. Skipping.\")\n",
    "            return None\n",
    "\n",
    "    # require 'position_inr'\n",
    "    if \"position_inr\" not in df.columns and \"position\" not in df.columns:\n",
    "        # try to infer from notional-like columns\n",
    "        possible = [c for c in df.columns if \"position\" in c or \"notional\" in c]\n",
    "        if possible:\n",
    "            df[\"position_inr\"] = df[possible[0]]\n",
    "        else:\n",
    "            print(f\"File {path} missing 'position_inr' column. Skipping.\")\n",
    "            return None\n",
    "\n",
    "    # compute 'equity' if missing (try capital_inr)\n",
    "    if \"equity\" not in df.columns:\n",
    "        if \"capital_inr\" in df.columns:\n",
    "            try:\n",
    "                initial = df[\"capital_inr\"].iloc[0]\n",
    "                df[\"equity\"] = df[\"capital_inr\"] / float(initial) if initial != 0 else df[\"capital_inr\"]\n",
    "            except Exception:\n",
    "                df[\"equity\"] = (1 + df[\"strategy_ret\"].fillna(0)).cumprod()\n",
    "        else:\n",
    "            df[\"equity\"] = (1 + df[\"strategy_ret\"].fillna(0)).cumprod()\n",
    "\n",
    "    # ensure numeric\n",
    "    for c in [\"strategy_ret\", \"position_inr\", \"equity\"]:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors='coerce')\n",
    "\n",
    "    # drop rows with NA index\n",
    "    df = df[~df.index.isna()]\n",
    "\n",
    "    if len(df) < MIN_DAYS_REQUIRED:\n",
    "        print(f\"File {path} has only {len(df)} rows (less than {MIN_DAYS_REQUIRED}) - skipping.\")\n",
    "        return None\n",
    "\n",
    "    # add pos sign column\n",
    "    df[\"pos\"] = np.sign(df[\"position_inr\"].fillna(0.0))\n",
    "\n",
    "    return df\n",
    "\n",
    "def event_window_slice(df, start, end):\n",
    "    \"\"\"Return df slice from start..end inclusive (dates are strings 'YYYY-MM-DD').\"\"\"\n",
    "    s = pd.to_datetime(start)\n",
    "    e = pd.to_datetime(end)\n",
    "    # inclusive slice using loc\n",
    "    try:\n",
    "        return df.loc[(df.index >= s) & (df.index <= e)].copy()\n",
    "    except Exception:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def make_event_plot(df_win, ticker, event_name, out_path):\n",
    "    \"\"\"\n",
    "    Create a plot for the event window:\n",
    "      - equity (left axis)\n",
    "      - pos sign (right axis as filled area)\n",
    "    Saves to out_path (PNG)\n",
    "    \"\"\"\n",
    "    if df_win.empty:\n",
    "        return False\n",
    "    fig, ax1 = plt.subplots(figsize=(10,4))\n",
    "    ax1.plot(df_win.index, df_win[\"equity\"], color=\"black\", linewidth=1.0, label=\"Equity (rel)\")\n",
    "    ax1.set_ylabel(\"Equity (relative)\")\n",
    "    ax1.set_title(f\"{ticker}: {event_name.replace('_', ' ')}\")\n",
    "\n",
    "    ax2 = ax1.twinx()\n",
    "    # draw pos as a filled area between -1 and 1 (scale to visible)\n",
    "    ax2.fill_between(df_win.index, df_win[\"pos\"], alpha=0.25, color=\"orange\", step='mid')\n",
    "    ax2.set_ylabel(\"Position sign\", color=\"orange\")\n",
    "\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    fig.tight_layout()\n",
    "    try:\n",
    "        fig.savefig(out_path, dpi=200)\n",
    "        plt.close(fig)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        plt.close(fig)\n",
    "        print(f\"Failed saving plot {out_path}: {e}\")\n",
    "        return False\n",
    "\n",
    "def compute_event_stats(df, start, end, pre_days=PRE_DAYS, post_days=POST_DAYS):\n",
    "    \"\"\"\n",
    "    Compute basic pre-event, event-window, post-event statistics.\n",
    "    Returns dict with numeric entries.\n",
    "    \"\"\"\n",
    "    s = pd.to_datetime(start)\n",
    "    e = pd.to_datetime(end)\n",
    "    win = df.loc[(df.index >= s) & (df.index <= e)]\n",
    "\n",
    "    # pre-window: last pre_days before s (strictly before s)\n",
    "    pre_df = df.loc[df.index < s].tail(pre_days)\n",
    "    # post-window: first post_days after e (strictly after e)\n",
    "    post_df = df.loc[df.index > e].head(post_days)\n",
    "\n",
    "    # compute event compounded return\n",
    "    if win.empty:\n",
    "        event_total = np.nan\n",
    "    else:\n",
    "        # product(1 + r) - 1\n",
    "        event_total = (1.0 + win[\"strategy_ret\"].fillna(0.0)).prod() - 1.0\n",
    "\n",
    "    return {\n",
    "        \"pre_avg_daily_ret\": float(pre_df[\"strategy_ret\"].mean()) if not pre_df.empty else np.nan,\n",
    "        \"event_total_ret\": float(event_total),\n",
    "        \"post_avg_daily_ret\": float(post_df[\"strategy_ret\"].mean()) if not post_df.empty else np.nan,\n",
    "        \"n_days_pre\": int(len(pre_df)),\n",
    "        \"n_days_event\": int(len(win)),\n",
    "        \"n_days_post\": int(len(post_df))\n",
    "    }\n",
    "\n",
    "# --------- MAIN BATCH PROCESS ----------\n",
    "def main():\n",
    "    os.makedirs(OUT_FIG_FOLDER, exist_ok=True)\n",
    "    # collect all backtest csv files\n",
    "    pattern = os.path.join(BACKTEST_FOLDER, \"*_backtest.csv\")\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        print(f\"No files found with pattern {pattern}. Check your BACKTEST_FOLDER and filenames.\")\n",
    "        return\n",
    "\n",
    "    rows = []  # summary rows\n",
    "    print(f\"Found {len(files)} backtest files. Running event analysis...\")\n",
    "\n",
    "    for path in files:\n",
    "        ticker = os.path.basename(path).replace(\"_backtest.csv\", \"\")\n",
    "        print(f\"\\nProcessing {ticker} ...\")\n",
    "\n",
    "        df = safe_read_backtest(path)\n",
    "        if df is None:\n",
    "            print(f\"Skipping {ticker} (could not read or insufficient data).\")\n",
    "            continue\n",
    "\n",
    "        # create ticker folder for plots\n",
    "        ticker_fig_dir = os.path.join(OUT_FIG_FOLDER, ticker.replace(\"/\", \"_\"))\n",
    "        os.makedirs(ticker_fig_dir, exist_ok=True)\n",
    "\n",
    "        for ev_name, (start, end) in EVENTS.items():\n",
    "            win_df = event_window_slice(df, start, end)\n",
    "            # save plot\n",
    "            out_png = os.path.join(ticker_fig_dir, f\"{ticker}_{ev_name}.png\")\n",
    "            plotted = make_event_plot(win_df, ticker, ev_name, out_png)\n",
    "\n",
    "            stats = compute_event_stats(df, start, end)\n",
    "            row = {\n",
    "                \"ticker\": ticker,\n",
    "                \"event\": ev_name,\n",
    "                \"event_start\": start,\n",
    "                \"event_end\": end,\n",
    "                \"plotted\": plotted,\n",
    "                **stats\n",
    "            }\n",
    "            rows.append(row)\n",
    "\n",
    "    # save summary CSV\n",
    "    if rows:\n",
    "        summary_df = pd.DataFrame(rows)\n",
    "        summary_df = summary_df.sort_values([\"event\", \"ticker\"]).reset_index(drop=True)\n",
    "        summary_df.to_csv(SUMMARY_CSV, index=False)\n",
    "        print(f\"\\nEvent analysis summary written to: {SUMMARY_CSV}\")\n",
    "    else:\n",
    "        print(\"No event rows to save.\")\n",
    "\n",
    "    print(\"\\nDone. Check the folder 'event_figs/' for plots and the CSV summary file.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2443,
     "status": "ok",
     "timestamp": 1763200354931,
     "user": {
      "displayName": "Ayush Singh",
      "userId": "10589746899026829773"
     },
     "user_tz": -330
    },
    "id": "qILMWgY07iz6",
    "outputId": "5340f467-8580-4c64-f3d1-8690ba5e2202"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote LaTeX snippet to: report_figs/event_analysis_section.tex\n",
      "Saved summary_sorted_by_cagr.csv\n",
      "Saved event_median_returns_by_ticker.csv\n",
      "\n",
      "All analysis outputs saved under: report_figs and momentum_results\n",
      "You can include the LaTeX snippet: report_figs/event_analysis_section.tex in your report (it escapes underscores).\n"
     ]
    }
   ],
   "source": [
    "# analysis_from_csv.py\n",
    "\"\"\"\n",
    "Analysis script that reads:\n",
    "  - momentum_results/summary.csv\n",
    "  - momentum_results/event_analysis_summary.csv\n",
    "\n",
    "Produces figures and tables under report_figs/ and momentum_results/.\n",
    "Generates a LaTeX snippet to include in your report with escaped underscores.\n",
    "\"\"\"\n",
    "import os\n",
    "import math\n",
    "import textwrap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -------- CONFIG --------\n",
    "BACKROOT = \"momentum_results\"\n",
    "SUMMARY_CSV = os.path.join(BACKROOT, \"summary.csv\")\n",
    "EVENT_SUMMARY_CSV = os.path.join(BACKROOT, \"event_analysis_summary.csv\")\n",
    "\n",
    "OUT_DIR = \"report_figs\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# analysis parameters\n",
    "TOP_N = 6   # top tickers to show in some plots\n",
    "LATEX_SNIPPET = os.path.join(OUT_DIR, \"event_analysis_section.tex\")\n",
    "\n",
    "# -------- helpers --------\n",
    "def safe_load_csv(path):\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"ERROR: missing file: {path}\")\n",
    "        return None\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR reading {path}: {e}\")\n",
    "        return None\n",
    "\n",
    "def savefig(fig, fname, dpi=200):\n",
    "    try:\n",
    "        fig.savefig(fname, dpi=dpi, bbox_inches=\"tight\")\n",
    "        plt.close(fig)\n",
    "    except Exception as e:\n",
    "        print(\"Failed to save\", fname, e)\n",
    "\n",
    "def escape_tex(s: str):\n",
    "    # escape underscores for \\texttt and file names\n",
    "    return s.replace(\"_\", \"\\\\_\")\n",
    "\n",
    "# -------- load data --------\n",
    "summary = safe_load_csv(SUMMARY_CSV)\n",
    "event = safe_load_csv(EVENT_SUMMARY_CSV)\n",
    "\n",
    "if summary is None or event is None:\n",
    "    raise SystemExit(\"Missing required CSVs. Run the backtest & event script first.\")\n",
    "\n",
    "# normalize column names\n",
    "summary.columns = [c.lower() for c in summary.columns]\n",
    "event.columns = [c.lower() for c in event.columns]\n",
    "\n",
    "# ensure numeric columns exist\n",
    "for col in [\"cagr\", \"sharpe\", \"total_return\", \"max_drawdown\"]:\n",
    "    if col not in summary.columns:\n",
    "        summary[col] = np.nan\n",
    "\n",
    "# basic distributions: CAGR histogram\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.hist(summary['cagr'].dropna(), bins=30, edgecolor='k')\n",
    "ax.set_title(\"Distribution of CAGR (per-ticker)\")\n",
    "ax.set_xlabel(\"CAGR\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "savefig(fig, os.path.join(OUT_DIR, \"hist_cagr.png\"))\n",
    "\n",
    "# scatter: sharpe vs cagr\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "ax.scatter(summary['cagr'], summary['sharpe'], alpha=0.7)\n",
    "ax.set_xlabel(\"CAGR\")\n",
    "ax.set_ylabel(\"Sharpe\")\n",
    "ax.grid(True, alpha=0.2)\n",
    "ax.set_title(\"Sharpe vs CAGR (per-ticker)\")\n",
    "savefig(fig, os.path.join(OUT_DIR, \"scatter_sharpe_cagr.png\"))\n",
    "\n",
    "# Top winners/losers overall\n",
    "summary_sorted = summary.sort_values(\"cagr\", ascending=False).reset_index(drop=True)\n",
    "top_winners = summary_sorted.head(TOP_N)\n",
    "top_losers = summary_sorted.tail(TOP_N).iloc[::-1]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.barh(top_winners[\"ticker\"].astype(str), top_winners[\"cagr\"], color=\"tab:blue\")\n",
    "ax.set_xlabel(\"CAGR\")\n",
    "ax.set_title(f\"Top {TOP_N} tickers by CAGR\")\n",
    "savefig(fig, os.path.join(OUT_DIR, \"topN_cagr.png\"))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,4))\n",
    "ax.barh(top_losers[\"ticker\"].astype(str), top_losers[\"cagr\"], color=\"tab:orange\")\n",
    "ax.set_xlabel(\"CAGR\")\n",
    "ax.set_title(f\"Bottom {TOP_N} tickers by CAGR\")\n",
    "savefig(fig, os.path.join(OUT_DIR, \"bottomN_cagr.png\"))\n",
    "\n",
    "# ---- Event-based aggregations ----\n",
    "# For each event, show top 10 winners and losers (by event_total_ret)\n",
    "if \"event\" not in event.columns or \"event_total_ret\" not in event.columns:\n",
    "    print(\"Event summary missing expected columns (event, event_total_ret).\")\n",
    "else:\n",
    "    events = sorted(event['event'].unique())\n",
    "    event_plots = []\n",
    "    for ev in events:\n",
    "        ev_df = event[event['event'] == ev].dropna(subset=[\"event_total_ret\"])\n",
    "        if ev_df.empty:\n",
    "            continue\n",
    "        ev_sorted = ev_df.sort_values(\"event_total_ret\", ascending=False).reset_index(drop=True)\n",
    "        top = ev_sorted.head(10)\n",
    "        bot = ev_sorted.tail(10)\n",
    "\n",
    "        # bar chart top winners\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        ax.barh(top['ticker'].astype(str), top['event_total_ret'], color=\"tab:green\")\n",
    "        ax.set_title(f\"Top 10 winners during {ev}\")\n",
    "        ax.set_xlabel(\"event total return\")\n",
    "        savefig(fig, os.path.join(OUT_DIR, f\"event_{ev}_top10.png\"))\n",
    "\n",
    "        # bar chart top losers\n",
    "        fig, ax = plt.subplots(figsize=(8,4))\n",
    "        ax.barh(bot['ticker'].astype(str), bot['event_total_ret'], color=\"tab:red\")\n",
    "        ax.set_title(f\"Top 10 losers during {ev}\")\n",
    "        ax.set_xlabel(\"event total return\")\n",
    "        savefig(fig, os.path.join(OUT_DIR, f\"event_{ev}_bottom10.png\"))\n",
    "\n",
    "        event_plots.append((ev, top, bot))\n",
    "\n",
    "    # summary table across events: median event return per ticker (pivot)\n",
    "    pivot = event.pivot_table(index=\"ticker\", columns=\"event\", values=\"event_total_ret\", aggfunc=\"median\")\n",
    "    # save pivot CSV\n",
    "    pivot.to_csv(os.path.join(BACKROOT, \"event_median_returns_by_ticker.csv\"))\n",
    "\n",
    "    # heatmap of median event returns for top tickers by overall CAGR (top 12)\n",
    "    top12 = summary_sorted.head(12)['ticker'].astype(str).tolist()\n",
    "    pivot_top12 = pivot.reindex(top12).fillna(0.0)\n",
    "\n",
    "    # plot heatmap with matplotlib (no seaborn dependency)\n",
    "    fig, ax = plt.subplots(figsize=(8,6))\n",
    "    im = ax.imshow(pivot_top12.values, aspect='auto', cmap='RdYlGn', vmin=-1, vmax=1)\n",
    "    ax.set_xticks(np.arange(len(pivot_top12.columns)))\n",
    "    ax.set_xticklabels([c.replace(\"_\", \" \") for c in pivot_top12.columns], rotation=45, ha='right')\n",
    "    ax.set_yticks(np.arange(len(pivot_top12.index)))\n",
    "    ax.set_yticklabels(pivot_top12.index)\n",
    "    ax.set_title(\"Median event returns (top 12 tickers by CAGR)\")\n",
    "    cbar = fig.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "    cbar.set_label(\"Median event return\")\n",
    "    savefig(fig, os.path.join(OUT_DIR, \"heatmap_median_event_returns_top12.png\"))\n",
    "\n",
    "# ---- Pre/Event/Post comparison for top N tickers ----\n",
    "N = 6\n",
    "topN = summary_sorted.head(N)['ticker'].astype(str).tolist()\n",
    "rows = []\n",
    "for t in topN:\n",
    "    # grab all events for ticker\n",
    "    te = event[event['ticker'].astype(str) == t]\n",
    "    for _, r in te.iterrows():\n",
    "        rows.append({\n",
    "            \"ticker\": t,\n",
    "            \"event\": r['event'],\n",
    "            \"n_days_pre\": r.get('n_days_pre', np.nan),\n",
    "            \"pre_avg_daily_ret\": r.get('pre_avg_daily_ret', np.nan),\n",
    "            \"event_total_ret\": r.get('event_total_ret', np.nan),\n",
    "            \"post_avg_daily_ret\": r.get('post_avg_daily_ret', np.nan)\n",
    "        })\n",
    "comp_df = pd.DataFrame(rows)\n",
    "comp_df.to_csv(os.path.join(BACKROOT, f\"top{N}_pre_event_post_table.csv\"), index=False)\n",
    "\n",
    "# save a simple latex table snippet for top N\n",
    "def df_to_latex_table(df, caption, label):\n",
    "    # uses simple tabular, escapes underscores\n",
    "    colnames = df.columns.tolist()\n",
    "    header = \" & \".join([escape_tex(c) for c in colnames]) + r\" \\\\\"\n",
    "    lines = [r\"\\begin{table}[H]\", r\"\\centering\", r\"\\small\", rf\"\\caption{{{caption}}}\", rf\"\\label{{{label}}}\", r\"\\begin{tabular}{%s}\" % (\"l\" * len(colnames)), r\"\\toprule\", header, r\"\\midrule\"]\n",
    "    for _, r in df.iterrows():\n",
    "        vals = []\n",
    "        for c in colnames:\n",
    "            v = r[c]\n",
    "            if pd.isna(v):\n",
    "                vals.append(\"\")\n",
    "            elif isinstance(v, (int, np.integer)):\n",
    "                vals.append(str(int(v)))\n",
    "            elif isinstance(v, (float, np.floating)):\n",
    "                vals.append(f\"{v:.4g}\")\n",
    "            else:\n",
    "                vals.append(escape_tex(str(v)))\n",
    "        lines.append(\" & \".join(vals) + r\" \\\\\")\n",
    "    lines.extend([r\"\\bottomrule\", r\"\\end{tabular}\", r\"\\end{table}\"])\n",
    "    return \"\\n\".join(lines)\n",
    "\n",
    "latex_tables = []\n",
    "if not comp_df.empty:\n",
    "    # keep only a few columns for readability\n",
    "    display_df = comp_df[[\"ticker\", \"event\", \"n_days_pre\", \"pre_avg_daily_ret\", \"event_total_ret\", \"post_avg_daily_ret\"]].copy()\n",
    "    # small formatting\n",
    "    latex_tables.append(df_to_latex_table(display_df.head(20), f\"Pre/Event/Post metrics for top {N} tickers\", \"tab:pre_event_topN\"))\n",
    "    # save snippet file\n",
    "    with open(LATEX_SNIPPET, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"% LaTeX snippet auto-generated by analysis_from_csv.py\\n\")\n",
    "        f.write(\"% Put this file into your report with \\\\input{report_figs/event_analysis_section.tex}\\n\\n\")\n",
    "        # include event figures (top winners/losers)\n",
    "        f.write(\"\\\\section*{Event-based Analysis}\\n\")\n",
    "        f.write(\"\\\\begin{figure}[H]\\n\\\\centering\\n\")\n",
    "        # include a couple of images if they exist\n",
    "        cand_imgs = [\n",
    "            \"hist_cagr.png\",\n",
    "            \"scatter_sharpe_cagr.png\",\n",
    "            \"topN_cagr.png\",\n",
    "            \"bottomN_cagr.png\",\n",
    "            \"heatmap_median_event_returns_top12.png\"\n",
    "        ]\n",
    "        for im in cand_imgs:\n",
    "            p = os.path.join(OUT_DIR, im)\n",
    "            if os.path.exists(p):\n",
    "                f.write(r\"\\includegraphics[width=0.48\\linewidth]{\" + escape_tex(p) + \"}\\n\")\n",
    "        f.write(\"\\\\caption{Selected diagnostic plots for event & per-ticker performance.}\\n\\\\end{figure}\\n\\n\")\n",
    "        # add topN table\n",
    "        f.write(latex_tables[0])\n",
    "        f.write(\"\\n\\n% End of auto-generated event analysis snippet\\n\")\n",
    "\n",
    "    print(\"Wrote LaTeX snippet to:\", LATEX_SNIPPET)\n",
    "else:\n",
    "    print(\"No pre/post comparison table generated (comp_df empty).\")\n",
    "\n",
    "# ---- Save additional CSVs for manual inspection ----\n",
    "summary_sorted.to_csv(os.path.join(BACKROOT, \"summary_sorted_by_cagr.csv\"), index=False)\n",
    "print(\"Saved summary_sorted_by_cagr.csv\")\n",
    "if 'pivot' in locals():\n",
    "    # pivot from earlier\n",
    "    pivot.fillna(0.0).to_csv(os.path.join(BACKROOT, \"event_median_returns_by_ticker.csv\"))\n",
    "    print(\"Saved event_median_returns_by_ticker.csv\")\n",
    "\n",
    "print(\"\\nAll analysis outputs saved under:\", OUT_DIR, \"and\", BACKROOT)\n",
    "print(\"You can include the LaTeX snippet:\", LATEX_SNIPPET, \"in your report (it escapes underscores).\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNGiepj0vRMtSG3bZysN+SP",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
